# Config for a simple 256 -> 16 autoencoder
model:
  model_id: dcae
  sample_size: [360,640]
  channels: 7
  latent_size: 8
  latent_channels: 128

  ch_0: 256
  ch_max: 2048

  encoder_blocks_per_stage: [4, 4, 4, 4, 4, 4]
  decoder_blocks_per_stage: [4, 4, 4, 4, 4, 4]

  use_middle_block: false
  
train:
  trainer_id: distill_enc
  data_id: s3_cod_features
  data_kwargs:
    bucket_name: cod-raw-360p-30fs
    prefix: rgb-depth-flow
    include_flow: true
    include_depth: true
    
  target_batch_size: 128
  batch_size: 16

  epochs: 100

  opt: AdamW
  opt_kwargs:
    lr: 1.0e-4
    weight_decay: 1.0e-4
    betas: [0.9, 0.95]
    eps: 1.0e-15

  lpips_id: convnext
  loss_weights:
    lpips: 10.0
    dwt: 1.0
    l1: 1.0

  scheduler: LinearWarmup
  scheduler_kwargs:
    warmup_steps: 1500
    min_lr: 1.0e-5

  checkpoint_dir: /mnt/data/checkpoints/owl_vaes/feats_distill_enc
  output_path: /mnt/data/checkpoints/owl_vaes/feats_distill_enc_output
  teacher_ckpt: /mnt/data/checkpoints/owl_vaes/cod_128x_feats/cod_128x_feats_160k_ema.pt
  teacher_cfg: configs/cod_128x_feats.yml
  resume_ckpt: null

  sample_interval: 500
  save_interval: 5000

  latent_scale: 1.0

wandb:
  name: ${env:WANDB_USER_NAME}
  project: new_vaes
  run_name: feats_distill_enc

